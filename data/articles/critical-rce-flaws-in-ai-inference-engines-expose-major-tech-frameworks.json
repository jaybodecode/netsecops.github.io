{
  "id": "816a7a0e-b79b-4d7d-8b86-7b2347102318",
  "slug": "critical-rce-flaws-in-ai-inference-engines-expose-major-tech-frameworks",
  "headline": "Critical RCE Flaws in AI Engines From Meta, NVIDIA, Microsoft Discovered",
  "title": "Critical RCE Flaws in AI Inference Engines Expose Major Tech Frameworks",
  "summary": "Security researchers have discovered critical remote code execution (RCE) vulnerabilities in widely used AI inference servers from major tech companies, including Meta, NVIDIA, and Microsoft, as well as open-source projects like vLLM. The vulnerabilities stem from the unsafe use of Python's 'pickle' module for data deserialization and exposed ZeroMQ (ZMQ) messaging endpoints. Exploitation could allow attackers to take full control of AI models and servers, posing a significant risk to enterprise AI infrastructure. Some flaws, termed 'Shadow Vulnerabilities,' remain unpatched in production environments.",
  "full_report": "## Executive Summary\nCritical remote code execution (RCE) vulnerabilities have been found in popular AI inference engines, affecting frameworks developed by **[Meta](https://www.meta.com/)**, **[NVIDIA](https://www.nvidia.com/)**, **[Microsoft](https://www.microsoft.com/)**, and several open-source projects. Research from Oligo Security identified systemic weaknesses related to insecure data deserialization via Python's `pickle` module and insecurely exposed ZeroMQ (ZMQ) messaging endpoints. Successful exploitation of these flaws could allow a remote attacker to execute arbitrary code on the AI server, leading to model theft, data poisoning, or a pivot into the broader corporate network. The discovery also highlights the issue of \"Shadow Vulnerabilities\"â€”known but unpatched flaws that persist in widely used code forks, such as in Microsoft's Sarathi-Serve, creating a hidden attack surface.\n\n---\n\n## Vulnerability Details\nThe core of the vulnerabilities lies in two primary insecure-by-default development practices common in the fast-moving AI space:\n\n1.  **Insecure Deserialization with `pickle`:** Many AI frameworks use Python's `pickle` module to serialize and deserialize data, including AI models and configurations. The `pickle` module is notoriously unsafe because it can execute arbitrary code when deserializing a maliciously crafted object. If an inference server accepts pickled data from an untrusted source, an attacker can send a malicious pickle payload to achieve RCE. This is a classic example of [`T1574 - Hijack Execution Flow`](https://attack.mitre.org/techniques/T1574/).\n\n2.  **Exposed ZeroMQ (ZMQ) Endpoints:** ZMQ is a high-performance asynchronous messaging library used for communication between different components of the AI stack. The researchers found that many frameworks expose ZMQ endpoints to the network without any authentication (e.g., HMAC or TLS). An attacker who can connect to this endpoint can use functions like `recv_pyobj()` to send a malicious Python object (often a pickled object), again leading to RCE.\n\n## Affected Systems\nThe vulnerabilities impact a wide range of popular AI/ML frameworks and servers, including:\n- **Proprietary Frameworks:** From vendors like Meta, NVIDIA, and Microsoft.\n- **Open-Source Projects:**\n  - `vLLM`\n  - `SGLang`\n  - `Modular`\n- **Unpatched Forks:** Microsoft's `Sarathi-Serve` is cited as an example of a project that inherited these vulnerabilities and remains unpatched, creating a \"Shadow Vulnerability.\"\n\n## Exploitation Status\nWhile the source articles do not mention active in-the-wild exploitation, the public disclosure and the ease of exploitation make it highly likely that threat actors will begin targeting these systems. The vulnerabilities are straightforward to exploit for an attacker with network access to the vulnerable endpoints. Many of the core open-source projects have released patches, but the risk remains high for downstream applications and unmaintained forks.\n\n## Impact Assessment\nA successful RCE attack on an AI inference server can have devastating consequences:\n- **Model Theft:** Attackers can steal proprietary, high-value AI models.\n- **Data Poisoning:** Malicious actors could manipulate the model's behavior or poison the training data, compromising the integrity of AI-driven decisions.\n- **Denial of Service:** Attackers could crash the AI service, disrupting business operations that rely on it.\n- **Network Pivot:** The compromised server can be used as a beachhead to attack other systems within the organization's network.\n- **Data Exfiltration:** Access to the server could expose sensitive data that is being processed by the AI model.\n\n## Cyber Observables for Detection\n- **Network Traffic:** Monitor for network traffic to and from AI servers on ZMQ ports (e.g., TCP/5555, 5556) that is not encrypted with TLS.\n- **Log Analysis:** Look for error messages in application logs related to deserialization failures or unexpected object types, which could indicate an exploitation attempt.\n- **Process Monitoring:** On AI servers, monitor for processes spawning unexpected child processes (e.g., a Python web service spawning a shell).\n\n## Detection Methods\n1.  **Code Scanning (SAST):** Use static application security testing (SAST) tools to scan Python codebases for unsafe usage of `pickle.load()` or `pickle.loads()` with data from untrusted sources. This is a form of [`File Analysis (D3-FA)`](https://d3fend.mitre.org/technique/d3f:FileAnalysis).\n2.  **Network Scanning:** Scan internal and external networks for open ZMQ ports and investigate any services that do not enforce authentication.\n3.  **Dependency Analysis:** Use Software Composition Analysis (SCA) tools to identify if your projects are using vulnerable versions of frameworks like `vLLM`.\n\n## Remediation Steps\n**For Developers:**\n1.  **Avoid `pickle`:** Do not use `pickle` to deserialize data from untrusted or unauthenticated sources. Use safer serialization formats like JSON for data interchange.\n2.  **Secure ZMQ:** Implement strong authentication mechanisms for all ZMQ communications. Use the built-in CurveZMQ or TLS to encrypt traffic and authenticate clients.\n\n**For Organizations:**\n1.  **Patch Immediately:** Update all affected AI frameworks (vLLM, SGLang, etc.) to the latest patched versions. This is a critical [`Software Update (D3-SU)`](https://d3fend.mitre.org/technique/d3f:SoftwareUpdate) action.\n2.  **Audit Forks:** If your organization uses forks of open-source AI projects (like Sarathi-Serve), audit them for these vulnerabilities and apply the necessary fixes manually.\n3.  **Network Segmentation:** Isolate AI inference servers in a segmented network and use firewalls to restrict access to their communication ports only to trusted clients.",
  "twitter_post": "ðŸš¨ Critical RCE flaws found in AI inference engines from Meta, NVIDIA, Microsoft & more. Unsafe 'pickle' use and exposed ZMQ endpoints allow server takeover. Patch your AI/ML frameworks now! ðŸ¤– #AI #CyberSecurity #RCE #Vulnerability",
  "meta_description": "Critical RCE vulnerabilities in AI inference engines from Meta, NVIDIA, Microsoft, and open-source projects like vLLM expose enterprise AI infrastructure to takeover via insecure deserialization ('pickle') and exposed ZMQ endpoints.",
  "category": [
    "Vulnerability",
    "Cloud Security",
    "Other"
  ],
  "severity": "critical",
  "entities": [
    {
      "name": "Meta",
      "type": "vendor",
      "url": "https://www.meta.com/"
    },
    {
      "name": "NVIDIA",
      "type": "vendor",
      "url": "https://www.nvidia.com/"
    },
    {
      "name": "Microsoft",
      "type": "vendor",
      "url": "https://www.microsoft.com/security"
    },
    {
      "name": "Oligo Security",
      "type": "security_organization"
    },
    {
      "name": "vLLM",
      "type": "product"
    },
    {
      "name": "SGLang",
      "type": "product"
    },
    {
      "name": "Modular",
      "type": "product"
    },
    {
      "name": "Sarathi-Serve",
      "type": "product"
    },
    {
      "name": "pickle",
      "type": "technology"
    },
    {
      "name": "ZeroMQ (ZMQ)",
      "type": "technology"
    }
  ],
  "cves": [],
  "sources": [
    {
      "url": "https://thehackernews.com/2025/11/critical-rce-flaws-in-ai-inference.html",
      "title": "Critical RCE Flaws in AI Inference Engines Expose Meta, Nvidia, and Microsoft Frameworks",
      "date": "2025-11-17",
      "friendly_name": "The Hacker News",
      "website": "thehackernews.com"
    },
    {
      "url": "https://www.securityweek.com/in-other-news-deepwatch-layoffs-macos-vulnerability-amazon-ai-bug-bounty/",
      "title": "In Other News: Deepwatch Layoffs, macOS Vulnerability, Amazon AI Bug Bounty",
      "date": "2025-11-14",
      "friendly_name": "SecurityWeek",
      "website": "securityweek.com"
    }
  ],
  "events": [],
  "mitre_techniques": [
    {
      "id": "T1574",
      "name": "Hijack Execution Flow",
      "tactic": "Defense Evasion"
    },
    {
      "id": "T1068",
      "name": "Exploitation for Privilege Escalation",
      "tactic": "Privilege Escalation"
    },
    {
      "id": "T1190",
      "name": "Exploit Public-Facing Application",
      "tactic": "Initial Access"
    }
  ],
  "mitre_mitigations": [
    {
      "id": "M1054",
      "name": "Software Configuration",
      "d3fend_techniques": [
        {
          "id": "D3-ACH",
          "name": "Application Configuration Hardening",
          "url": "https://d3fend.mitre.org/technique/d3f:ApplicationConfigurationHardening"
        }
      ],
      "description": "Harden the configuration of AI frameworks to disable unsafe features and enforce authentication on communication channels.",
      "domain": "enterprise"
    },
    {
      "id": "M1051",
      "name": "Update Software",
      "d3fend_techniques": [
        {
          "id": "D3-SU",
          "name": "Software Update",
          "url": "https://d3fend.mitre.org/technique/d3f:SoftwareUpdate"
        }
      ],
      "description": "Update affected AI frameworks to patched versions that address the deserialization and ZMQ vulnerabilities.",
      "domain": "enterprise"
    },
    {
      "id": "M1048",
      "name": "Application Isolation and Sandboxing",
      "d3fend_techniques": [
        {
          "id": "D3-DA",
          "name": "Dynamic Analysis",
          "url": "https://d3fend.mitre.org/technique/d3f:DynamicAnalysis"
        }
      ],
      "description": "Run AI inference servers in isolated or sandboxed environments to limit the impact of a potential compromise.",
      "domain": "enterprise"
    }
  ],
  "d3fend_countermeasures": [],
  "iocs": [],
  "cyber_observables": [
    {
      "type": "port",
      "value": "5555",
      "description": "Default or common port used by ZeroMQ (ZMQ). Unauthenticated access to this port on an AI server is a strong indicator of vulnerability.",
      "context": "Network vulnerability scans, firewall logs.",
      "confidence": "medium"
    },
    {
      "type": "string_pattern",
      "value": "pickle.load",
      "description": "Presence of the unsafe 'pickle.load' or 'pickle.loads' function in Python source code, which can be a source of insecure deserialization vulnerabilities.",
      "context": "Static Application Security Testing (SAST), manual code review.",
      "confidence": "high"
    },
    {
      "type": "command_line_pattern",
      "value": "recv_pyobj()",
      "description": "Use of the ZMQ function 'recv_pyobj()' without proper authentication, which allows receiving and processing arbitrary Python objects, leading to RCE.",
      "context": "Static Application Security Testing (SAST), manual code review.",
      "confidence": "high"
    }
  ],
  "tags": [
    "AI",
    "Machine Learning",
    "RCE",
    "Insecure Deserialization",
    "pickle",
    "vLLM",
    "NVIDIA",
    "Meta",
    "Microsoft"
  ],
  "extract_datetime": "2025-11-16T15:00:00.000Z",
  "article_type": "TechArticle",
  "impact_scope": {
    "geographic_scope": "global",
    "industries_affected": [
      "Technology"
    ],
    "other_affected": [
      "Organizations utilizing AI/ML frameworks",
      "Open-source software users"
    ]
  },
  "pub_date": "2025-11-16",
  "reading_time_minutes": 6,
  "createdAt": "2025-11-16T15:00:00.000Z",
  "updatedAt": "2025-11-16T15:00:00.000Z"
}