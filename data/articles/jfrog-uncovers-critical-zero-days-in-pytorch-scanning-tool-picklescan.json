{
  "id": "cc6d4d51-018a-4910-9707-a0a35a3b0efd",
  "slug": "jfrog-uncovers-critical-zero-days-in-pytorch-scanning-tool-picklescan",
  "headline": "Critical Zero-Days in PyTorch Scanner 'PickleScan' Create AI Supply Chain Risk",
  "title": "JFrog Discloses Critical Zero-Day Vulnerabilities in PyTorch Security Tool PickleScan, Enabling Arbitrary Code Execution",
  "summary": "Security firm JFrog has disclosed three critical zero-day vulnerabilities in PickleScan, a popular open-source tool used to scan Python pickle files for malware, particularly within the PyTorch AI framework. The flaws, collectively rated with a CVSS score of 9.3, allow an attacker to craft a malicious AI model that bypasses PickleScan's security checks. When this seemingly safe model is loaded by a user, it can lead to arbitrary code execution. This discovery, announced on December 3, 2025, highlights a significant software supply chain risk for the AI/ML community, as attackers could distribute weaponized models that evade standard security scanning.",
  "full_report": "## Executive Summary\nOn December 3, 2025, the **[JFrog](https://jfrog.com/)** security research team revealed the discovery of three critical zero-day vulnerabilities in **PickleScan**, a widely adopted open-source tool for detecting malicious Python pickle files. These vulnerabilities carry a **CVSS score of 9.3 (Critical)** and introduce a severe software supply chain risk for the Artificial Intelligence and Machine Learning (AI/ML) ecosystem. An attacker can exploit these flaws to create a malicious AI model, often used with the **[PyTorch](https://pytorch.org/)** framework, that PickleScan will incorrectly flag as safe. When an unsuspecting developer or organization loads this trojanized model, it can trigger arbitrary code execution on their system. This attack vector allows for the covert distribution of malware through public model repositories, bypassing a key security control in the MLOps pipeline.\n\n---\n\n## Vulnerability Details\nThe vulnerabilities lie in the logic of PickleScan itself. The tool is designed to statically analyze a `pickle` fileâ€”a common format for serializing Python objects, heavily used for saving and loading AI modelsâ€”to identify dangerous opcodes that could lead to code execution. The flaws discovered by JFrog represent bypass techniques, where a specially crafted pickle file can be constructed to appear benign to PickleScan's scanner while still containing a malicious payload that is executed upon deserialization by a standard Python pickle loader.\n\nThis creates a dangerous gap in the security supply chain: an organization may believe it is safely handling untrusted models by scanning them with PickleScan, but in reality, it remains vulnerable to exploitation. The end result is **arbitrary code execution** on the machine that loads the model, which could be a developer's workstation, a training server, or a production inference server.\n\n## Affected Systems\n- **Product:** PickleScan (all versions prior to a potential patch)\n- **Ecosystem:** Any individual or organization that uses PickleScan to vet untrusted `pickle` files or PyTorch models (`.pt` files).\n\nThis is not a vulnerability in PyTorch itself, but in a security tool designed to protect its users. However, the vast popularity of PyTorch makes the impact of a faulty scanner particularly widespread.\n\n## Exploitation Status\nThese are zero-day vulnerabilities, meaning they were not publicly known before JFrog's disclosure and no patches were available at the time of announcement. While there is no public evidence of active exploitation in the wild, the disclosure of the technical details means that threat actors could quickly weaponize these bypass techniques. The risk is especially high for organizations that automatically pull and deploy models from public repositories like Hugging Face.\n\n## Impact Assessment\nThis vulnerability represents a critical threat to the security of the AI/ML software supply chain. A successful exploit could lead to:\n- **Compromise of Development Environments:** Attackers could gain control of researcher or developer machines, stealing proprietary code, data, or credentials.\n- **Production Server Takeover:** If a malicious model is deployed to production, the attacker could compromise the inference servers, potentially stealing sensitive input data, manipulating model outputs, or using the servers as a pivot point into the broader corporate network.\n- **AI Model Poisoning or Backdooring:** An attacker could use the code execution vulnerability to subtly alter the behavior of the model itself, creating a backdoor that is triggered by specific inputs.\n\nThis undermines the trust in shared AI models and highlights the immaturity of security tooling in the rapidly evolving MLOps space.\n\n## Detection Methods\n- **Static Analysis Limitations:** The core issue is that static analysis tools like PickleScan can be bypassed. Relying solely on them for security is insufficient.\n- **Dynamic Analysis (Sandboxing):** The most effective way to detect a malicious model is to load it in a heavily sandboxed and monitored environment. Observe the model's behavior during loading and inference for suspicious activities like network connections, file system access, or process creation. This is an application of D3FEND's [`D3-DA: Dynamic Analysis`](https://d3fend.mitre.org/technique/d3f:DynamicAnalysis).\n- **Model Provenance:** Whenever possible, only use models from trusted, verified sources. Check for digital signatures or other attestations of a model's origin.\n\n## Remediation Steps\n1.  **Assume Untrusted Models are Malicious:** Until a patched version of PickleScan or a more robust alternative is available, organizations should treat all AI models from untrusted sources as potentially malicious.\n2.  **Use Sandboxing:** Do not load or deserialize untrusted pickle files on production systems or sensitive developer workstations. Use isolated, ephemeral environments (e.g., containers with no network access and read-only file systems) for initial model inspection.\n3.  **Seek Alternatives:** Explore alternative model formats that have a safer deserialization process, such as `safetensors`.\n4.  **Monitor for Updates:** Keep a close watch on the PickleScan project repository for any patches or mitigation guidance from the maintainers.",
  "twitter_post": "ðŸš¨ CRITICAL ZERO-DAYS: JFrog found 3 critical flaws (CVSS 9.3) in the 'PickleScan' tool used for PyTorch. Malicious AI models can bypass scans and cause RCE. A major AI supply chain risk! ðŸ¤– #AISecurity #PyTorch #SupplyChain #ZeroDay",
  "meta_description": "JFrog has discovered three critical zero-day vulnerabilities (CVSS 9.3) in PickleScan, a tool for scanning PyTorch AI models, allowing malicious models to bypass security and execute code.",
  "category": [
    "Vulnerability",
    "Supply Chain Attack",
    "Cloud Security"
  ],
  "severity": "critical",
  "entities": [
    {
      "name": "JFrog",
      "type": "vendor",
      "url": "https://jfrog.com/"
    },
    {
      "name": "PickleScan",
      "type": "product"
    },
    {
      "name": "PyTorch",
      "type": "technology",
      "url": "https://pytorch.org/"
    },
    {
      "name": "Python",
      "type": "technology"
    }
  ],
  "cves": [],
  "sources": [
    {
      "url": "https://www.mysecuritymedia.com/3-zero-day-vulnerabilities-found-in-picklescan/",
      "title": "3 Zero Day Vulnerabilities Found in PickleScan - Australian Cyber Security Magazine",
      "date": "2025-12-03",
      "friendly_name": "Australian Cyber Security Magazine",
      "website": "mysecuritymedia.com"
    },
    {
      "url": "https://www.hipther.com/2025/12/03/cybersecurity-roundup-partnerships-funding-and-emerging-threats-december-3-2025/",
      "title": "Cybersecurity Roundup: Partnerships, Funding, and Emerging Threats â€“ December 3, 2025",
      "date": "2025-12-03",
      "friendly_name": "Hipther",
      "website": "hipther.com"
    }
  ],
  "events": [
    {
      "datetime": "2025-12-03",
      "summary": "JFrog publicly discloses the three zero-day vulnerabilities in PickleScan."
    }
  ],
  "mitre_techniques": [
    {
      "id": "T1199",
      "name": "Trusted Relationship",
      "tactic": "Initial Access"
    },
    {
      "id": "T1059.006",
      "name": "Python",
      "tactic": "Execution"
    },
    {
      "id": "T1559",
      "name": "Inter-Process Communication",
      "tactic": "Execution"
    }
  ],
  "mitre_mitigations": [
    {
      "id": "M1048",
      "name": "Application Isolation and Sandboxing",
      "description": "Loading untrusted AI models in a sandboxed, isolated environment is the most effective way to contain potential code execution.",
      "domain": "enterprise"
    },
    {
      "id": "M1021",
      "name": "Restrict Web-Based Content",
      "description": "Be highly selective about the sources of AI models, preferring official, signed models from trusted repositories over unverified ones.",
      "domain": "enterprise"
    }
  ],
  "d3fend_countermeasures": [
    {
      "technique_id": "D3-DA",
      "technique_name": "Dynamic Analysis",
      "url": "https://d3fend.mitre.org/technique/d3f:DynamicAnalysis",
      "recommendation": "Since static analysis with PickleScan is proven to be unreliable, organizations must shift to dynamic analysis for untrusted models. Before a model is used, it should be loaded in a secure, isolated sandbox (e.g., a minimal Docker container with gVisor or a dedicated VM). This environment should have networking disabled and strict file system permissions. Monitor the deserialization process for any suspicious system calls, file I/O, or process creation attempts. If the model loading process triggers any behavior beyond expected memory allocation and computation, it should be flagged as malicious and rejected. This approach moves from trusting a scanner's verdict to a 'distrust and verify' model for AI supply chain security.",
      "mitre_mitigation_id": "M1048"
    },
    {
      "technique_id": "D3-ACH",
      "technique_name": "Application Configuration Hardening",
      "url": "https://d3fend.mitre.org/technique/d3f:ApplicationConfigurationHardening",
      "recommendation": "A key strategic mitigation is to reduce reliance on the inherently unsafe pickle format. Development and MLOps teams should prioritize migrating to safer model serialization formats like `safetensors`. This format is designed specifically to prevent arbitrary code execution during loading. Mandate the use of `safetensors` in your organization's MLOps policies for all new models. For existing models, create a plan to convert them. While this is a longer-term effort, it addresses the root cause of the problem rather than just trying to detect malicious pickles. It hardens the application stack against an entire class of vulnerability.",
      "mitre_mitigation_id": "M1054"
    }
  ],
  "iocs": [],
  "cyber_observables": [
    {
      "type": "file_name",
      "value": "*.pt",
      "description": "PyTorch models are often saved with a .pt extension and use the Python pickle format, making them a vector for this vulnerability.",
      "context": "File Integrity Monitoring, Asset Inventory",
      "confidence": "high"
    },
    {
      "type": "process_name",
      "value": "python",
      "description": "Monitor the Python process that loads a model for anomalous behavior, such as unexpected network callbacks, file creation, or spawning of child processes.",
      "context": "EDR, Sandboxed Analysis Environment",
      "confidence": "high"
    },
    {
      "type": "other",
      "value": "safetensors",
      "description": "A secure alternative format for storing tensors that is not vulnerable to arbitrary code execution on loading. Its presence indicates a safer practice.",
      "context": "MLOps Pipeline Security",
      "confidence": "high"
    }
  ],
  "tags": [
    "Zero-Day",
    "AI Security",
    "MLOps",
    "PyTorch",
    "PickleScan",
    "JFrog",
    "Supply Chain Attack",
    "RCE"
  ],
  "extract_datetime": "2025-12-04T15:00:00.000Z",
  "article_type": "Advisory",
  "impact_scope": {
    "geographic_scope": "global",
    "industries_affected": [
      "Technology",
      "Finance",
      "Healthcare"
    ],
    "other_affected": [
      "AI/ML researchers and developers",
      "Organizations using MLOps pipelines"
    ]
  },
  "pub_date": "2025-12-04",
  "reading_time_minutes": 5,
  "createdAt": "2025-12-04T15:00:00.000Z",
  "updatedAt": "2025-12-04T15:00:00.000Z"
}